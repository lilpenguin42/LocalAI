{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lilpenguin42/LocalAI/blob/master/integrations/haystack/haystack_lfqa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOckGkQ_Nzgo"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/integrations/haystack/haystack_lfqa.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaazjsqpV0dB"
      },
      "source": [
        "# Abstractive QA with LFQA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BrhVtyhV7wg"
      },
      "source": [
        "[*Find the full article version of this notebook here*](https://www.pinecone.io/learn/haystack-lfqa/)\n",
        "\n",
        "We have seen incredible breakthroughs in Natural Language Processing (NLP) in the last several years. [Question Answering](https://www.pinecone.io/learn/question-answering/) (QA) systems that leverage popular language models such as BERT, ROBERTA, etc., can now easily answer questions from a given context with great precision. These QA systems accept a question, locate the most relevant document passages containing answers from a document store, and extract or generate the most likely answer.\n",
        "\n",
        "Recent studies have focused on more advanced QA systems such as Long-Form Question Answering (LFQA) systems that can generate multi-sentenced abstractive (generated) answers to open-ended questions. It works by searching massive document stores for documents containing relevant information and then using this information to compose an accurate multi-sentence answer synthetically. The relevant documents give larger context for generating original, abstractive long-form answers.\n",
        "\n",
        "At present, searching for information on a topic is painstaking. For instance, we might multiple queries on Google or any other search engine to pull snippets of information from several sources. LFQA simplifies this by pulling info from several sources and compressing them into a single, human-like answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SfduZPFWBco"
      },
      "source": [
        "## Using Haystack for LFQA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeNnTi8qWLvW"
      },
      "source": [
        "We will use [Haystack](https://www.pinecone.io/docs/integrations/haystack/) to build a LFQA system. Three main components are needed to build a LFQA pipeline in Haystack: *DocumentStore, Retriever, and Generator.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJj4Q8ZtWTBQ"
      },
      "source": [
        "### DocumentStore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccPXUHRsWWZO"
      },
      "source": [
        "As the name suggests, the document store is where all our documents are stored. Haystack has different document stores we can use for various use cases. For LFQA, we will use a dense/embedding-based retriever, but it is possible to use traditional methods such as TF–IDF and BM25. So, we need a vector-optimized document store to hold embedding vectors that represent our documents. We will use the *PineconeDocumentStore*, now available in Haystack starting from [version 1.3.0](https://github.com/deepset-ai/haystack/releases/tag/v1.3.0). We could have easily used any other vector-optimized document store such as *FAISSDocumentStore*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKNb2ex8Wcot"
      },
      "source": [
        "### Retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVPvzMkcWnz4"
      },
      "source": [
        "The QA system needs information relevant to the query to generate an answer to the question. So, we need to retrieve the documents containing relevant information from the document store. The retriever’s job is to find the best candidates by computing the similarity between the question and the document embeddings. The final answer is generated based on the best candidates.\n",
        "\n",
        "We will use Haystack’s *EmbeddingRetriever* in our LFQA pipeline. It works by first generating the query embedding using a language model and then computing the dot product or cosine similarity between the document embeddings in the document store. Then, the top-k most relevant documents are retrieved. We will use a SentenceTransformer model fine-tuned for the query/document matching task as the retriever.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR8S2z-oW45K"
      },
      "source": [
        "### Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gTowxNbW2mf"
      },
      "source": [
        "We will use ELI5 BART for the generator - a sequence-to-sequence model trained using the ‘Explain Like I’m 5’ (ELI5) dataset. Sequence-to-Sequence models can take a text sequence as input and produce a different text sequence as the output.\n",
        "\n",
        "The input to the ELI5 BART model is a single string which is a concatenation of the query and the relevant documents providing the context for the answer. The documents are separated by a special token &lt;P>, so the input string will look as follows:\n",
        "\n",
        ">question: What is a sonic boom? context: &lt;P> A sonic boom is a sound associated with shock waves created when an object travels through the air faster than the speed of sound. &lt;P> Sonic booms generate enormous amounts of sound energy, sounding similar to an explosion or a thunderclap to the human ear. &lt;P> Sonic booms due to large supersonic aircraft can be particularly loud and startling, tend to awaken people, and may cause minor damage to some structures. This led to prohibition of routine supersonic flight overland.\n",
        "\n",
        "We will use Haystack’s *Seq2SeqGenerator* - a generic sequence-to-sequence generator based on HuggingFace's transformers library, to initialize the BART model. When using *Seq2SeqGenerator* the concatenation process above is automatically handled by the haystack and transformers library. The generator will compose a paragraph-long answer based on the relevant context documents.\n",
        "\n",
        "More detail on how the ELI5 dataset was built is available [here](https://arxiv.org/abs/1907.09190) and how ELI5 BART model was trained is available [here](https://yjernite.github.io/lfqa.html).\n",
        "\n",
        "\n",
        "Now let's build our LFQA system using Haystack."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-vHGLnqX4Pn"
      },
      "source": [
        "## Preparing the Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ts1F6fu3dMZ",
        "outputId": "1839d2c8-5632-427b-864a-035fa72d2bac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Oct 17 22:45:38 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Make sure you have a GPU running to speed up things.\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GThnZdT63k2s"
      },
      "source": [
        "Install the required libraries and their dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVulpqxmrCWP",
        "outputId": "65090960-64dc-4b35-af47-73408e7208ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (6.0.1)\n",
            "Requirement already satisfied: loguru>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (0.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.7.1)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (1.26.16)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (2023.7.22)\n",
            "Requirement already satisfied: farm-haystack[inference] in /usr/local/lib/python3.10/dist-packages (1.19.0)\n",
            "Requirement already satisfied: boilerpy3 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[inference]) (1.0.6)\n",
            "Requirement already satisfied: canals==0.3.2 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[inference]) (0.3.2)\n",
            "Requirement already satisfied: events in /usr/local/lib/python3.10/dist-packages (from farm-haystack[inference]) (0.5)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from farm-haystack[inference]) (4.3.3)\n",
            "Requirement already satisfied: lazy-imports==0.3.1 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[inference]) (0.3.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from farm-haystack[inference]) (9.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from farm-haystack[inference]) (3.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from farm-haystack[inference]) (1.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from farm-haystack[inference]) (9.4.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from farm-haystack[inference]) (3.9.1)\n",
            "Requirement already satisfied: posthog in /usr/local/lib/python3.10/dist-packages (from farm-haystack[inference]) (3.0.1)\n",
            "Requirement already satisfied: prompthub-py==4.0.0 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[inference]) (4.0.0)\n",
            "Requirement already satisfied: pydantic<2 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[inference]) (1.10.12)\n",
            "Requirement already satisfied: quantulum3 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[inference]) (0.9.0)\n",
            "Requirement already satisfied: rank-bm25 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[inference]) (0.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from farm-haystack[inference]) (2.31.0)\n",
            "Requirement already satisfied: requests-cache<1.0.0 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[inference]) (0.9.8)\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[inference]) (1.3.0)\n",
            "Requirement already satisfied: sseclient-py in /usr/local/lib/python3.10/dist-packages (from farm-haystack[inference]) (1.7.2)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from farm-haystack[inference]) (8.2.2)\n",
            "Requirement already satisfied: tiktoken>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[inference]) (0.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from farm-haystack[inference]) (4.65.0)\n",
            "Requirement already satisfied: transformers==4.31.0 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[inference]) (4.31.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[inference]) (0.16.4)\n",
            "Requirement already satisfied: sentence-transformers>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[inference]) (2.2.2)\n",
            "Requirement already satisfied: pyyaml<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from prompthub-py==4.0.0->farm-haystack[inference]) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0->farm-haystack[inference]) (3.12.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0->farm-haystack[inference]) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0->farm-haystack[inference]) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0->farm-haystack[inference]) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0->farm-haystack[inference]) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0->farm-haystack[inference]) (0.3.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0->farm-haystack[inference]) (2.0.1+cu118)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0->farm-haystack[inference]) (0.21.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0->farm-haystack[inference]) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0->farm-haystack[inference]) (3.20.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.5.0->farm-haystack[inference]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.5.0->farm-haystack[inference]) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack[inference]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack[inference]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack[inference]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack[inference]) (2023.7.22)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from requests-cache<1.0.0->farm-haystack[inference]) (1.4.4)\n",
            "Requirement already satisfied: attrs>=21.2 in /usr/local/lib/python3.10/dist-packages (from requests-cache<1.0.0->farm-haystack[inference]) (23.1.0)\n",
            "Requirement already satisfied: cattrs>=22.2 in /usr/local/lib/python3.10/dist-packages (from requests-cache<1.0.0->farm-haystack[inference]) (23.1.2)\n",
            "Requirement already satisfied: url-normalize>=1.4 in /usr/local/lib/python3.10/dist-packages (from requests-cache<1.0.0->farm-haystack[inference]) (1.4.3)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->farm-haystack[inference]) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->farm-haystack[inference]) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->farm-haystack[inference]) (3.2.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.0->farm-haystack[inference]) (0.15.2+cu118)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.0->farm-haystack[inference]) (3.8.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->farm-haystack[inference]) (0.19.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->farm-haystack[inference]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->farm-haystack[inference]) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog->farm-haystack[inference]) (1.16.0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog->farm-haystack[inference]) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog->farm-haystack[inference]) (2.2.1)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from quantulum3->farm-haystack[inference]) (6.0.5)\n",
            "Requirement already satisfied: num2words in /usr/local/lib/python3.10/dist-packages (from quantulum3->farm-haystack[inference]) (0.5.12)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers==4.31.0->farm-haystack[inference]) (5.9.5)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from cattrs>=22.2->requests-cache<1.0.0->farm-haystack[inference]) (1.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers==4.31.0->farm-haystack[inference]) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers==4.31.0->farm-haystack[inference]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers==4.31.0->farm-haystack[inference]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers==4.31.0->farm-haystack[inference]) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers==4.31.0->farm-haystack[inference]) (16.0.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=2.2.0->farm-haystack[inference]) (8.1.6)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from num2words->quantulum3->farm-haystack[inference]) (0.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers==4.31.0->farm-haystack[inference]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers==4.31.0->farm-haystack[inference]) (1.3.0)\n",
            "/bin/bash: line 1: conda: command not found\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pinecone-client\n",
        "!pip install -U 'farm-haystack[pinecone]'>=1.8.0\n",
        "!pip install -U farm-haystack[inference]\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxpan1D1a2gd"
      },
      "source": [
        "## Initializing the PineconeDocumentStore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0x_P2jAbvPu"
      },
      "source": [
        "We need an API key to use the PineconeDocumentStore in Haystack (you can sign up for free [here](https://app.pinecone.io/) and get an API key)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MyzvG346a5Z3"
      },
      "outputs": [],
      "source": [
        "from haystack.document_stores import PineconeDocumentStore\n",
        "\n",
        "document_store = PineconeDocumentStore(\n",
        "    api_key='8b048cbc-fc64-42b2-a731-39890e921b12',\n",
        "    index='haystack-lfqa',\n",
        "    environment='gcp-starter',\n",
        "    similarity=\"cosine\",\n",
        "    embedding_dim=768\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Rv9J3lDGt4Kp",
        "outputId": "ca6ce9fb-0c31-41ee-ff98-7531ff29d738"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cosine'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "document_store.metric_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLRKiIN_cfob",
        "outputId": "5d04c9fd-155e-4404-a5a2-68f559e7b39f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "document_store.get_document_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8c62cWvmLdX",
        "outputId": "fae00e47-511b-4b45-d28a-6a35aa905a5b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "document_store.get_embedding_count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GV_618Q1g-fs"
      },
      "source": [
        "The above code is all we need to initialize a Pinecone document store with Haystack. It will either create a Pinecone index named ```haystack-lfqa``` if it is not already there or connect to an existing index with the same name. The embedding dimension is set to 768 as the SentenceTransformer model we use to encode queries and documents outputs a vector with 768 dimensions. We also set the similarity metric to cosine as this particular model was trained to be used with cosine similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaCnEvLFg-lg"
      },
      "source": [
        "## Preparing and Indexing Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XweCiHNnjQRn"
      },
      "source": [
        "We will use the Wiki Snippets dataset, containing over 17 million passages from Wikipedia, as our source documents. But for this demo, we will use only fifty thousand passages which contains 'History' in the 'section_title' column as indexing the whole dataset will take a lot of time. But feel free to use the entire dataset if you wish. Pinecone vector database can easily handle millions of documents for you. This dataset is available on HuggingFace, so we can use the HuggingFace dataset library to load the dataset quickly and filter the historical passages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6PBceFOi-uD",
        "outputId": "fcb4e44e-2417-404c-b8a5-63aa9de3dc64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<datasets.iterable_dataset.IterableDataset at 0x7eb0dcd633a0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "wiki_data = load_dataset(\n",
        "    'vblagoje/wikipedia_snippets_streamed',\n",
        "    split='train',\n",
        "    streaming=True\n",
        ")\n",
        "wiki_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkpdlVGWgajI"
      },
      "source": [
        "We are loading the dataset in the streaming mode so that we don't have to wait for the whole dataset to download (which is over 9GB). Instead, we access the data when we iterate through the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5v7HrmxfRa1",
        "outputId": "f34ba302-5ecc-4302-caa0-3df198e4bb23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'wiki_id': 'Q7593707',\n",
              " 'start_paragraph': 2,\n",
              " 'start_character': 0,\n",
              " 'end_paragraph': 6,\n",
              " 'end_character': 511,\n",
              " 'article_title': \"St John the Baptist's Church, Atherton\",\n",
              " 'section_title': 'History',\n",
              " 'passage_text': \"St John the Baptist's Church, Atherton History There have been three chapels or churches on the site of St John the Baptist parish church. The first chapel at Chowbent was built in 1645 by John Atherton as a chapel of ease of Leigh Parish Church. It was sometimes referred to as the Old Bent Chapel. It was not consecrated and used by the Presbyterians as well as the Vicar of Leigh. In 1721 Lord of the manor Richard Atherton expelled the dissenters who subsequently built Chowbent Chapel. The first chapel was consecrated in 1723 by the Bishop of Sodor and\"}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# show the contents of a single document in the dataset\n",
        "next(iter(wiki_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyIMo-tS30KY",
        "outputId": "8c42c395-2e46-42cb-dee6-d7d6b72fc5d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<datasets.iterable_dataset.IterableDataset at 0x7eb0dcd52110>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Filter only documents with History as section_title\n",
        "history = wiki_data.filter(lambda d: d['section_title'].startswith('History'))\n",
        "history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SWIkD4trMtv"
      },
      "source": [
        "Now the dataset is ready, we need to initialize the second component in our LFQA system - the Retriever."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk_LOW8quyzA"
      },
      "source": [
        "## Initializing the Retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W32cUk2JvC8q"
      },
      "source": [
        "We will use Haystack's *EmbeddingRetriever* with a SentenceTransformer model trained based on Microsoft's MPNet. This model performs quite well for comparing the similarity between queries and documents. We can use the retriever to easily compute and update the embeddings for all the documents in the document store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESKK9I1Vkk73",
        "outputId": "0b3e97c0-2151-4bac-b6ee-ac555931375d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import torch\n",
        "# confirm GPU is available (if using CPU this step will be slower)\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJhwAHGvx2Ei"
      },
      "source": [
        "It will take some time to compute all the embeddings and update the index. If you have access to a GPU, it will significantly speed up the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BZmgZ3gzdM8M"
      },
      "outputs": [],
      "source": [
        "from haystack.nodes import EmbeddingRetriever\n",
        "\n",
        "retriever = EmbeddingRetriever(\n",
        "    document_store=document_store,\n",
        "    embedding_model=\"flax-sentence-embeddings/all_datasets_v3_mpnet-base\",\n",
        "    model_format=\"sentence_transformers\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXP4zUh3phWv"
      },
      "source": [
        "To index the documents, we first create Haystack Document objects containing the content and metadata for each document. We are iterating through the filtered dataset and adding the documents to the document store when `256` Document objects and embeddings are created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "3a279b60eef24d5abec13f2b08562e63",
            "706df96b09f84fc38b15b5831df4273d",
            "ec82dc60520246af8fb794dcc7ccf769",
            "c42cca09607d483187a8e5be8d705b48",
            "6780e11c4aae41f3b5466a11f1e92d16",
            "1a40718a43284e00b2bc2e651e5f0993",
            "83865f63b82d423a88eb5541e5f9b501",
            "dc24b64d989041d7ba5467047bcc3a8a",
            "f7774756197c4896abffb5149d9070cf",
            "149d3fd6634e4d71acbf509ef48d85dc",
            "1c1efddd074b4e84a1bd8f85f0e41086",
            "c5fabcf90bde46e68bc070bc582805a3",
            "60c321138a854adc84dac8d4c4662d15",
            "761821576ec44153abc1b1626c0e7f80",
            "3d642acdeee44638b61859ee0366bb22",
            "a54111f495c543b28a9d77039981a969",
            "ca7873eec2824cc08e9af6659f45b3e3",
            "f32ee2625bc746e586f728b8b382cdd2",
            "33400fab3b2a4a1ca550c9e4d52f7dd7",
            "a1bacd3c01de4e269dc16286646b10af",
            "9519bcc87d8e4d0eb109c7c7879acf86",
            "481cf0c04e8f42969c11eca2f5a9b0dd"
          ],
          "height": 531
        },
        "id": "v1i4jTHAnVyq",
        "outputId": "e44ba635-8e33-4f58-a321-b5c7bcb79791"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a279b60eef24d5abec13f2b08562e63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5fabcf90bde46e68bc070bc582805a3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Writing Documents:   0%|          | 0/256 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ApiException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mApiException\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d1c6aa794d61>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mdocument_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtotal_doc_count\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/haystack/document_stores/pinecone.py\u001b[0m in \u001b[0;36mwrite_documents\u001b[0;34m(self, documents, index, batch_size, duplicate_documents, headers, labels)\u001b[0m\n\u001b[1;32m    433\u001b[0m                     \u001b[0mdata_to_write_to_pinecone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                     \u001b[0;31m# Metadata fields and embeddings are stored in Pinecone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpinecone_indexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_to_write_to_pinecone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m                     \u001b[0;31m# Add IDs to ID list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_local_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/utils/error_handling.py\u001b[0m in \u001b[0;36minner_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# raises exceptions in case of invalid config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mMaxRetryError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProtocolError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/index.py\u001b[0m in \u001b[0;36mupsert\u001b[0;34m(self, vectors, namespace, batch_size, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_upsert_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_check_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/index.py\u001b[0m in \u001b[0;36m_upsert_batch\u001b[0;34m(self, vectors, namespace, _check_type, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid vector value passed: cannot interpret type {type(item)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         return self._vector_api.upsert(\n\u001b[0m\u001b[1;32m    238\u001b[0m             UpsertRequest(\n\u001b[1;32m    239\u001b[0m                 \u001b[0mvectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_vector_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/client/api_client.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \"\"\"\n\u001b[0;32m--> 776\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_http_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/client/api/vector_operations_api.py\u001b[0m in \u001b[0;36m__upsert\u001b[0;34m(self, upsert_request, **kwargs)\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'upsert_request'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m                 \u001b[0mupsert_request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_with_http_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         self.upsert = _Endpoint(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/client/api_client.py\u001b[0m in \u001b[0;36mcall_with_http_info\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    836\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'header'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Content-Type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheader_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m         return self.api_client.call_api(\n\u001b[0m\u001b[1;32m    839\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'endpoint_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'http_method'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/client/api_client.py\u001b[0m in \u001b[0;36mcall_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \"\"\"\n\u001b[1;32m    412\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0masync_req\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m             return self.__call_api(resource_path, method,\n\u001b[0m\u001b[1;32m    414\u001b[0m                                    \u001b[0mpath_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m                                    \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/client/api_client.py\u001b[0m in \u001b[0;36m__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mApiException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/client/api_client.py\u001b[0m in \u001b[0;36m__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# perform request and return response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             response_data = self.request(\n\u001b[0m\u001b[1;32m    201\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0mpost_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpost_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/client/api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    457\u001b[0m                                             body=body)\n\u001b[1;32m    458\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             return self.rest_client.POST(url,\n\u001b[0m\u001b[1;32m    460\u001b[0m                                          \u001b[0mquery_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m                                          \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/client/rest.py\u001b[0m in \u001b[0;36mPOST\u001b[0;34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    269\u001b[0m     def POST(self, url, headers=None, query_params=None, post_params=None,\n\u001b[1;32m    270\u001b[0m              body=None, _preload_content=True, _request_timeout=None):\n\u001b[0;32m--> 271\u001b[0;31m         return self.request(\"POST\", url,\n\u001b[0m\u001b[1;32m    272\u001b[0m                             \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                             \u001b[0mquery_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/client/rest.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mServiceException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_resp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mApiException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_resp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mApiException\u001b[0m: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'content-type': 'application/json', 'Content-Length': '124', 'date': 'Thu, 03 Aug 2023 01:27:43 GMT', 'x-envoy-upstream-service-time': '2', 'server': 'envoy', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\nHTTP response body: {\"code\":3,\"message\":\"The requested feature 'Namespaces' is not supported by the current index type 'Starter'.\",\"details\":[]}\n"
          ]
        }
      ],
      "source": [
        "from haystack import Document\n",
        "from tqdm.auto import tqdm  # progress bar\n",
        "\n",
        "total_doc_count = 50000\n",
        "batch_size = 256\n",
        "\n",
        "counter = 0\n",
        "docs = []\n",
        "for d in tqdm(history, total=total_doc_count):\n",
        "    # create haystack document object with text content and doc metadata\n",
        "    doc = Document(\n",
        "        content=d[\"passage_text\"],\n",
        "        meta={\n",
        "            \"article_title\": d[\"article_title\"],\n",
        "            'section_title': d['section_title']\n",
        "        }\n",
        "    )\n",
        "    docs.append(doc)\n",
        "    counter += 1\n",
        "    if counter % batch_size == 0:\n",
        "        # writing docs everytime `batch_size` docs are reached\n",
        "        embeds = retriever.embed_documents(docs)\n",
        "        for i, doc in enumerate(docs):\n",
        "            doc.embedding = embeds[i]\n",
        "        document_store.write_documents(docs)\n",
        "        docs.clear()\n",
        "    if counter == total_doc_count:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jjuvvGOEUCJ",
        "outputId": "eb9b0933-08a0-464b-f4e6-60a7beb91c4e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "49915"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "document_store.get_embedding_count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSvBuUfIxXso"
      },
      "source": [
        "The embeddings are updated to the document store, and our retriever is now ready. Let's test the retriever before we use it in the LFQA pipeline. We can test queries with the retriever by loading it into a *DocumentSearchPipeline*. Keep in mind we are only using fifty thousand passages from the Wiki Snippets dataset, so it is likely that documents containing relevant information for our exact queries are not in the document store. You can always test whether relevant documents are returned by running some queries on *DocumentSearchPipeline*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550,
          "referenced_widgets": [
            "1f7614a5e4e4475782b726ad0be545a5"
          ]
        },
        "id": "5AY24GCFyObj",
        "outputId": "e77ba2f3-a2a5-430a-abaa-194dc30cc40c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f7614a5e4e4475782b726ad0be545a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: When was the first electric power system built?\n",
            "\n",
            "{   'content': 'Electric power system History In 1881, two electricians built '\n",
            "               \"the world's first power system at Godalming in England. It was \"\n",
            "               'powered by two waterwheels and produced an alternating current '\n",
            "               'that in turn supplied seven Siemens arc lamps at 250 volts and '\n",
            "               '34 incandescent lamps at 40 volts. However, supply to the '\n",
            "               'lamps was intermittent and in 1882 Thomas Edison and his '\n",
            "               'company, The Edison Electric Light Company, developed the '\n",
            "               'first steam-powered electric power station on Pearl Street in '\n",
            "               'New York City. The Pearl Street Station initially powered '\n",
            "               'around 3,000 lamps for 59 customers. The power station '\n",
            "               'generated direct current and',\n",
            "    'name': None}\n",
            "\n",
            "{   'content': 'by a coal burning steam engine, and it started generating '\n",
            "               'electricity on September 4, 1882, serving an initial load of '\n",
            "               '400 incandescent lamps used by 85 customers located within '\n",
            "               'about 2 miles (3.2\\xa0km) of the station. \\n'\n",
            "               'However, with the advent of AC, there came the use of '\n",
            "               'transformers to convert the generated power to a much higher '\n",
            "               'voltage for transmission allowed the power plants and users to '\n",
            "               'be separated by hundreds of miles if needed. The high voltage '\n",
            "               'could then use transformers to obtain lower voltages for final '\n",
            "               'use. Single point failures were minimized in the plant design. '\n",
            "               'The AC',\n",
            "    'name': None}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from haystack.pipelines import DocumentSearchPipeline\n",
        "from haystack.utils import print_documents\n",
        "\n",
        "search_pipe = DocumentSearchPipeline(retriever)\n",
        "result = search_pipe.run(\n",
        "    query=\"When was the first electric power system built?\",\n",
        "    params={\"Retriever\": {\"top_k\": 2}}\n",
        ")\n",
        "\n",
        "print_documents(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0_Camne0E0n"
      },
      "source": [
        "As you can see, the retriever can find relevant documents from the document store. Now let's initialize the third compontent in our LFQA system - the Generator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkZ3G-Lj2-RS"
      },
      "source": [
        "## Initializing the Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYBeibfQ3JaU"
      },
      "source": [
        "For the generator we will load Haystack’s generic *Seq2SeqGenerator* with a model trained specifically for LFQA. We could use bart_lfqa by [vblagoje](https://huggingface.co/vblagoje) or bart_eli5 by [yjernite](https://huggingface.co/yjernite), both models performs quite well. bart_lfqa was trained with a newer ELI5 dataset, so we will go with that. For more details about the new ELI5 dataset, please refer to this [article](https://towardsdatascience.com/long-form-qa-beyond-eli5-an-updated-dataset-and-approach-319cb841aabb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vnVNXAH1R2c"
      },
      "outputs": [],
      "source": [
        "from haystack.nodes import Seq2SeqGenerator\n",
        "\n",
        "generator = Seq2SeqGenerator(model_name_or_path=\"vblagoje/bart_lfqa\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTmHQvKP8s4v"
      },
      "source": [
        "## Initializing a Generative QA Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtVBvB0t8zjU"
      },
      "source": [
        "Finally, we need to add the retriever and generator to Haystack's *GenerativeQAPipeline*, a ready-made pipeline for generative QA task. The *GenerativeQAPipeline*, as you might expect, combines the retriever with the generator to produce answers to our questions, and it is the primary interface for communicating with our LFQA system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osloQQS09bgR"
      },
      "outputs": [],
      "source": [
        "from haystack.pipelines import GenerativeQAPipeline\n",
        "\n",
        "pipe = GenerativeQAPipeline(generator, retriever)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-nmJGH3KxhO"
      },
      "source": [
        "## Asking Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyqgGp7WApd3"
      },
      "source": [
        "Now let's run some queries in our LFQA system. When queying we can specificy the number of documents we want the retriever to retrieve and the number of answers the generator to produce. The final answers will be generated based on the documents retrieved from the document store.\n",
        "\n",
        "The code below is what you need to run queries in the LFQA system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745,
          "referenced_widgets": [
            "d3e83fe6d9834679be8a494ef9c9cc88"
          ]
        },
        "id": "QrSrcSQFK4AH",
        "outputId": "4e42b5c6-a9e9-47f6-bfbe-b2f0a4a1779c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3e83fe6d9834679be8a494ef9c9cc88",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'what was the war of currents?',\n",
              " 'answers': [<Answer {'answer': \"The War of Currents was the rivalry between Thomas Edison and George Westinghouse's companies over which form of transmission (direct or alternating current) was superior.\", 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_id': None, 'meta': {'doc_ids': ['3a43249b33b1435e94ef9b22f01989b6', '54f12cf9010626d589b22712a1547983', 'ef560b97430a71ba9ff193062d6a9d0b'], 'doc_scores': [0.711440012, 0.6970715000000001, 0.6848947255], 'content': ['consultant at the Westinghouse Electric & Manufacturing Company\\'s Pittsburgh labs.\\nBy 1888, the electric power industry was flourishing, and power companies had built thousands of power systems (both direct and alternating current) in the United States and Europe. These networks were effectively dedicated to providing electric lighting. During this time the rivalry between Thomas Edison and George Westinghouse\\'s companies had grown into a propaganda campaign over which form of transmission (direct or alternating current) was superior, a series of events known as the \"War of Currents\". In 1891, Westinghouse installed the first major power system that was designed to drive a', 'of the British administration a favorite route for the smuggling of slaves.', 'of migration began, this state of affairs sometimes led to international incidents, with countries of origin refusing to recognize the new nationalities of natives who had migrated, and when possible, conscripting natives who had naturalized as citizens of another country into military service. The most notable example was the War of 1812, triggered by British impressment of American seamen who were alleged to be British subjects into naval service.\\nIn the aftermath of the 1867 Fenian Rising, Irish-Americans who had gone to Ireland to participate in the uprising and were caught were charged with treason, as the British authorities considered them'], 'titles': ['', '', '']}}>],\n",
              " 'documents': [<Document: {'content': 'consultant at the Westinghouse Electric & Manufacturing Company\\'s Pittsburgh labs.\\nBy 1888, the electric power industry was flourishing, and power companies had built thousands of power systems (both direct and alternating current) in the United States and Europe. These networks were effectively dedicated to providing electric lighting. During this time the rivalry between Thomas Edison and George Westinghouse\\'s companies had grown into a propaganda campaign over which form of transmission (direct or alternating current) was superior, a series of events known as the \"War of Currents\". In 1891, Westinghouse installed the first major power system that was designed to drive a', 'content_type': 'text', 'score': 0.711440012, 'meta': {'article_title': 'Electric power system', 'section_title': 'History'}, 'embedding': None, 'id': '3a43249b33b1435e94ef9b22f01989b6'}>,\n",
              "  <Document: {'content': 'of the British administration a favorite route for the smuggling of slaves.', 'content_type': 'text', 'score': 0.6970715000000001, 'meta': {'article_title': 'Muri, Nigeria', 'section_title': 'History'}, 'embedding': None, 'id': '54f12cf9010626d589b22712a1547983'}>,\n",
              "  <Document: {'content': 'of migration began, this state of affairs sometimes led to international incidents, with countries of origin refusing to recognize the new nationalities of natives who had migrated, and when possible, conscripting natives who had naturalized as citizens of another country into military service. The most notable example was the War of 1812, triggered by British impressment of American seamen who were alleged to be British subjects into naval service.\\nIn the aftermath of the 1867 Fenian Rising, Irish-Americans who had gone to Ireland to participate in the uprising and were caught were charged with treason, as the British authorities considered them', 'content_type': 'text', 'score': 0.6848947255, 'meta': {'article_title': 'Multiple citizenship', 'section_title': 'History'}, 'embedding': None, 'id': 'ef560b97430a71ba9ff193062d6a9d0b'}>],\n",
              " 'root_node': 'Query',\n",
              " 'params': {'Retriever': {'top_k': 3}, 'Generator': {'top_k': 1}},\n",
              " 'node_id': 'Generator'}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = pipe.run(\n",
        "        query=\"what was the war of currents?\",\n",
        "        params={\n",
        "            \"Retriever\": {\"top_k\": 3},\n",
        "            \"Generator\": {\"top_k\": 1}\n",
        "        })\n",
        "\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASYeTQwvpez6"
      },
      "source": [
        "We can clean up the output using Haystack's `print_answers` util."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "8a21272c9edb4c1ab5062c0e8cb77b24"
          ]
        },
        "id": "e3S-XEIBpez6",
        "outputId": "9059abde-c552-4fd9-dcb0-f4ef2cbf2f61"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a21272c9edb4c1ab5062c0e8cb77b24",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: what was the war of currents?\n",
            "Answers:\n",
            "[   {   'answer': 'The War of Currents was the rivalry between Thomas Edison '\n",
            "                  \"and George Westinghouse's companies over which form of \"\n",
            "                  'transmission (direct or alternating current) was superior.'}]\n"
          ]
        }
      ],
      "source": [
        "from haystack.utils import print_answers\n",
        "\n",
        "result = pipe.run(\n",
        "        query=\"what was the war of currents?\",\n",
        "        params={\n",
        "            \"Retriever\": {\"top_k\": 3},\n",
        "            \"Generator\": {\"top_k\": 1}\n",
        "        })\n",
        "\n",
        "print_answers(result, details=\"minimum\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhvNB3k2pez6"
      },
      "source": [
        "The answer here is good although there is not too much detail. When we find an answer is either not good or lacking detail there can be two combining factors for this:\n",
        "\n",
        "* The generator model has not been trained on data that includes information about the *\"war on currents\"* and so it has not *memorized* this information within it's model weights.\n",
        "\n",
        "* We have not returned any contexts that contain the answer, so the generator has no reliable external sources of information.\n",
        "\n",
        "If neither of these conditions are satisfied, the generator cannot produce a factually correct answer. However, in our case we are returning some good external context. We can try and return more detail by increasing the number of contexts retrieved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "bc0bd74d1c184c1bafc24fd1d0883c7a"
          ]
        },
        "id": "rOVOuhnDpez7",
        "outputId": "810a0906-96dc-43ef-91a5-1aac20c3fd2d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc0bd74d1c184c1bafc24fd1d0883c7a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: what was the war of currents?\n",
            "Answers:\n",
            "[   {   'answer': 'The War of Currents was the rivalry between Thomas Edison '\n",
            "                  \"and George Westinghouse's companies over which form of \"\n",
            "                  'transmission (direct or alternating current) was superior.'}]\n"
          ]
        }
      ],
      "source": [
        "result = pipe.run(\n",
        "        query=\"what was the war of currents?\",\n",
        "        params={\n",
        "            \"Retriever\": {\"top_k\": 10},\n",
        "            \"Generator\": {\"top_k\": 1}\n",
        "        })\n",
        "\n",
        "print_answers(result, details=\"minimum\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJgJXijJpez7"
      },
      "source": [
        "Now we're seeing much more info. Some of it rambles but for the most part it is relevant. We can also compare these results to generator created answer *without* any context by querying the generator directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq3aqm1Ypez7",
        "outputId": "ebc1c844-427a-4243-dd78-8d6b94010530"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: what was the war of currents?\n",
            "Answers:\n",
            "[{'answer': 'I\\'m not sure what you mean by \"war\".'}]\n"
          ]
        }
      ],
      "source": [
        "result = generator.predict(\n",
        "    query=\"what was the war of currents?\",\n",
        "    documents=[Document(content=\"\")],\n",
        "    top_k=1\n",
        ")\n",
        "\n",
        "print_answers(result, details=\"minimum\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1BtGOuApez7"
      },
      "source": [
        "Clearly, the retrieved contexts are important. Although this isn't always the case, for example if we ask a more well-known question..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9DmGgcFpez7",
        "outputId": "cf7b1eb9-dcad-47f1-94f1-24494ced4a16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: who was the first person on the moon?\n",
            "Answers:\n",
            "[{'answer': 'The first man to walk on the moon was Neil Armstrong.'}]\n"
          ]
        }
      ],
      "source": [
        "result = generator.predict(\n",
        "    query=\"who was the first person on the moon?\",\n",
        "    documents=[Document(content=\"\")],\n",
        "    top_k=1\n",
        ")\n",
        "\n",
        "print_answers(result, details=\"minimum\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEinxJ_Apez7"
      },
      "source": [
        "For this type of general knowledge, the generator model is able to pull the answer directly from it's own *\"memory\"*, eg the model weights optimized during training, where it will have been given training data containing this information. Larger models have a larger memory, but when asking more specific questions (like our question about the war on currents) we rarely return good answers without an external data source."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcxKyad5pez7"
      },
      "source": [
        "Let's try some more questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "eac6dfa40f81435f83759732378643e2"
          ]
        },
        "id": "HK2VTAdYpez7",
        "outputId": "d498c79e-8c05-4033-b24a-604cf5469bfb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eac6dfa40f81435f83759732378643e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: when was the first electric power system built?\n",
            "Answers:\n",
            "[   {   'answer': 'The first electric power system was built in 1881 at '\n",
            "                  'Godalming in England. It was powered by two waterwheels and '\n",
            "                  'produced an alternating current that in turn supplied seven '\n",
            "                  'Siemens arc lamps at 250 volts and 34 incandescent lamps at '\n",
            "                  '40 volts.'}]\n"
          ]
        }
      ],
      "source": [
        "result = pipe.run(\n",
        "        query=\"when was the first electric power system built?\",\n",
        "        params={\n",
        "            \"Retriever\": {\"top_k\": 3},\n",
        "            \"Generator\": {\"top_k\": 1}\n",
        "        })\n",
        "\n",
        "print_answers(result, details=\"minimum\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6SVfFGepez7"
      },
      "source": [
        "We can confirm the correctness of this answer by checking the contexts that this answer has been built from:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObmeSu_-pez7",
        "outputId": "607770c1-10c1-4566-ced3-3752ed65459e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Electric power system History In 1881, two electricians built the world's first power system at Godalming in England. It was powered by two waterwheels and produced an alternating current that in turn supplied seven Siemens arc lamps at 250 volts and 34 incandescent lamps at 40 volts. However, supply to the lamps was intermittent and in 1882 Thomas Edison and his company, The Edison Electric Light Company, developed the first steam-powered electric power station on Pearl Street in New York City. The Pearl Street Station initially powered around 3,000 lamps for 59 customers. The power station generated direct current and\n",
            "---\n",
            "by a coal burning steam engine, and it started generating electricity on September 4, 1882, serving an initial load of 400 incandescent lamps used by 85 customers located within about 2 miles (3.2 km) of the station. \n",
            "However, with the advent of AC, there came the use of transformers to convert the generated power to a much higher voltage for transmission allowed the power plants and users to be separated by hundreds of miles if needed. The high voltage could then use transformers to obtain lower voltages for final use. Single point failures were minimized in the plant design. The AC\n",
            "---\n",
            "consultant at the Westinghouse Electric & Manufacturing Company's Pittsburgh labs.\n",
            "By 1888, the electric power industry was flourishing, and power companies had built thousands of power systems (both direct and alternating current) in the United States and Europe. These networks were effectively dedicated to providing electric lighting. During this time the rivalry between Thomas Edison and George Westinghouse's companies had grown into a propaganda campaign over which form of transmission (direct or alternating current) was superior, a series of events known as the \"War of Currents\". In 1891, Westinghouse installed the first major power system that was designed to drive a\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "for doc in result['documents']:\n",
        "    print(doc.content, end='\\n---\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J4fb_2Tpez8"
      },
      "source": [
        "In some cases the generator will generate a false answer if it is asked about a topic and does not recieve any relevant contexts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159,
          "referenced_widgets": [
            "e4ad4e58a8474ad4a99377868b6a603a"
          ]
        },
        "id": "QudhKU-9QX8N",
        "outputId": "f63f8fb0-3992-40ec-e409-911f8d06e4c7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4ad4e58a8474ad4a99377868b6a603a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: where did COVID-19 originate?\n",
            "Answers:\n",
            "[   {   'answer': 'COVID-19 is a zoonotic disease, which means that it is a '\n",
            "                  'virus that is transmitted from one animal to another. This '\n",
            "                  'means that there is no way to know for sure where it came '\n",
            "                  'from.'}]\n"
          ]
        }
      ],
      "source": [
        "result = pipe.run(\n",
        "        query=\"where did COVID-19 originate?\",\n",
        "        params={\n",
        "            \"Retriever\": {\"top_k\": 3},\n",
        "            \"Generator\": {\"top_k\": 1}\n",
        "        })\n",
        "\n",
        "print_answers(result, details=\"minimum\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdjDZP-KXtiL"
      },
      "source": [
        "This is one drawback of the LFQA pipeline, although this can be mitigated to an extent by implementing thresholds on answer confidence scores, and including the sources behind any generated answers. Let's finish with a few final questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124,
          "referenced_widgets": [
            "505d438ef96246c98dee55b3589eaf0f"
          ]
        },
        "id": "JPMbVHkCpez8",
        "outputId": "757b047f-a0cf-4bc4-f5cd-e24d1badf07d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "505d438ef96246c98dee55b3589eaf0f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: what was NASAs most expensive project?\n",
            "Answers:\n",
            "[   {   'answer': 'The Space Shuttle was the most expensive project in the '\n",
            "                  'history of NASA. It cost over $100 billion to build.'}]\n"
          ]
        }
      ],
      "source": [
        "result = pipe.run(\n",
        "    query=\"what was NASAs most expensive project?\",\n",
        "    params={\n",
        "        \"Retriever\": {\"top_k\": 3},\n",
        "        \"Generator\": {\"top_k\": 1}\n",
        "    }\n",
        ")\n",
        "\n",
        "print_answers(result, details=\"minimum\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195,
          "referenced_widgets": [
            "a772a43fd3ee4c89a471b730bb716407"
          ]
        },
        "id": "ZznXSQy2YkYp",
        "outputId": "b31ed64a-ef88-4e60-8649-023969c4964e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a772a43fd3ee4c89a471b730bb716407",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: tell me something interesting about the history of Earth?\n",
            "Answers:\n",
            "[   {   'answer': \"I'm not sure if this is what you're looking for, but I've \"\n",
            "                  \"always been fascinated by the fact that the Earth's \"\n",
            "                  'magnetic field is so weak compared to the rest of the solar '\n",
            "                  'system. The magnetic field of the Earth is about 1/10th the '\n",
            "                  'strength of that of the strongest magnetic field in the '\n",
            "                  'Solar System.'}]\n"
          ]
        }
      ],
      "source": [
        "result = pipe.run(\n",
        "        query=\"tell me something interesting about the history of Earth?\",\n",
        "        params={\n",
        "            \"Retriever\": {\"top_k\": 3},\n",
        "            \"Generator\": {\"top_k\": 1}\n",
        "        })\n",
        "\n",
        "print_answers(result, details=\"minimum\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "5bc0e35b9157404aa1d0a8aa61919f43"
          ]
        },
        "id": "Gtaj8eoLpez8",
        "outputId": "110b258d-e942-4cb9-84dc-b79f62645a87"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5bc0e35b9157404aa1d0a8aa61919f43",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: who created the Nobel prize and why?\n",
            "Answers:\n",
            "[   {   'answer': 'The Nobel Prize was created by Alfred Nobel in his will in '\n",
            "                  '1896. The idea was that he would use his fortune to create '\n",
            "                  'a series of prizes for those who confer the \"greatest '\n",
            "                  'benefit on mankind\" in physics, chemistry, physiology or '\n",
            "                  'medicine, literature, and peace.'}]\n"
          ]
        }
      ],
      "source": [
        "result = pipe.run(\n",
        "        query=\"who created the Nobel prize and why?\",\n",
        "        params={\n",
        "            \"Retriever\": {\"top_k\": 10},\n",
        "            \"Generator\": {\"top_k\": 1}\n",
        "        })\n",
        "\n",
        "print_answers(result, details=\"minimum\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195,
          "referenced_widgets": [
            "abc84949fca84ae6897a3243c0ee8b96"
          ]
        },
        "id": "34nvmT1Ypez8",
        "outputId": "f4c7b6b5-2ab9-4678-fa1f-081236b24731"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "abc84949fca84ae6897a3243c0ee8b96",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: how is the nobel prize funded?\n",
            "Answers:\n",
            "[   {   'answer': 'The Nobel Prizes are awarded by the Swedish Academy of '\n",
            "                  'Sciences and the Norwegian Nobel Committee. The Swedish '\n",
            "                  'Academy is made up of members of the Royal Swedish Academy, '\n",
            "                  'the Norwegian Academy, and the American Academy of Arts and '\n",
            "                  'Sciences. The Nobel Foundation is a non-profit organization '\n",
            "                  \"that is funded by Alfred Nobel's personal fortune.\"}]\n"
          ]
        }
      ],
      "source": [
        "result = pipe.run(\n",
        "        query=\"how is the nobel prize funded?\",\n",
        "        params={\n",
        "            \"Retriever\": {\"top_k\": 10},\n",
        "            \"Generator\": {\"top_k\": 1}\n",
        "        })\n",
        "\n",
        "print_answers(result, details=\"minimum\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJyV4JS19Gmn"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "haystack_lfqa.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "environment": {
      "kernel": "python3",
      "name": "common-cu110.m91",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cu110:m91"
    },
    "interpreter": {
      "hash": "e81a84c338879f0412495ea98350e80595740634d3ce0fba8d30f35c60f1a4c3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3a279b60eef24d5abec13f2b08562e63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_706df96b09f84fc38b15b5831df4273d",
              "IPY_MODEL_ec82dc60520246af8fb794dcc7ccf769",
              "IPY_MODEL_c42cca09607d483187a8e5be8d705b48"
            ],
            "layout": "IPY_MODEL_6780e11c4aae41f3b5466a11f1e92d16"
          }
        },
        "706df96b09f84fc38b15b5831df4273d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a40718a43284e00b2bc2e651e5f0993",
            "placeholder": "​",
            "style": "IPY_MODEL_83865f63b82d423a88eb5541e5f9b501",
            "value": "  1%"
          }
        },
        "ec82dc60520246af8fb794dcc7ccf769": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc24b64d989041d7ba5467047bcc3a8a",
            "max": 50000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7774756197c4896abffb5149d9070cf",
            "value": 255
          }
        },
        "c42cca09607d483187a8e5be8d705b48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_149d3fd6634e4d71acbf509ef48d85dc",
            "placeholder": "​",
            "style": "IPY_MODEL_1c1efddd074b4e84a1bd8f85f0e41086",
            "value": " 255/50000 [00:18&lt;11:51, 69.95it/s]"
          }
        },
        "6780e11c4aae41f3b5466a11f1e92d16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a40718a43284e00b2bc2e651e5f0993": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83865f63b82d423a88eb5541e5f9b501": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc24b64d989041d7ba5467047bcc3a8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7774756197c4896abffb5149d9070cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "149d3fd6634e4d71acbf509ef48d85dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c1efddd074b4e84a1bd8f85f0e41086": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5fabcf90bde46e68bc070bc582805a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_60c321138a854adc84dac8d4c4662d15",
              "IPY_MODEL_761821576ec44153abc1b1626c0e7f80",
              "IPY_MODEL_3d642acdeee44638b61859ee0366bb22"
            ],
            "layout": "IPY_MODEL_a54111f495c543b28a9d77039981a969"
          }
        },
        "60c321138a854adc84dac8d4c4662d15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca7873eec2824cc08e9af6659f45b3e3",
            "placeholder": "​",
            "style": "IPY_MODEL_f32ee2625bc746e586f728b8b382cdd2",
            "value": "Batches: 100%"
          }
        },
        "761821576ec44153abc1b1626c0e7f80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33400fab3b2a4a1ca550c9e4d52f7dd7",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1bacd3c01de4e269dc16286646b10af",
            "value": 8
          }
        },
        "3d642acdeee44638b61859ee0366bb22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9519bcc87d8e4d0eb109c7c7879acf86",
            "placeholder": "​",
            "style": "IPY_MODEL_481cf0c04e8f42969c11eca2f5a9b0dd",
            "value": " 8/8 [00:02&lt;00:00,  3.61it/s]"
          }
        },
        "a54111f495c543b28a9d77039981a969": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca7873eec2824cc08e9af6659f45b3e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f32ee2625bc746e586f728b8b382cdd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33400fab3b2a4a1ca550c9e4d52f7dd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1bacd3c01de4e269dc16286646b10af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9519bcc87d8e4d0eb109c7c7879acf86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "481cf0c04e8f42969c11eca2f5a9b0dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}